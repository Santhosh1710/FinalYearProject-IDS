# -*- coding: utf-8 -*-
"""Intrustion Detection System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RUTK2M449TRuJZMbQCiqz1BZBodjWQQG

# **Data Preprocessing**
"""

# Install gdown if not already installed
!pip install -q gdown

import gdown

# importing required libraries
import numpy as np
import pandas as pd
import seaborn as sns

import pickle # saving and loading trained model
from os import path

# importing required libraries for normalizing data
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

# importing library for plotting
import matplotlib.pyplot as plt

# importing library for support vector machine classifier
from sklearn.svm import SVC
# importing library for K-neares-neighbor classifier
from sklearn.neighbors import KNeighborsClassifier
# importing library for Linear Discriminant Analysis Model
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# importing library for Quadratic Discriminant Analysis Model
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

from sklearn import metrics
from sklearn.metrics import accuracy_score # for calculating accuracy of model
from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc

from keras.layers import Dense # importing dense layer
from keras.models import Sequential #importing Sequential layer
from keras.models import model_from_json # saving and loading trained model
from keras.layers import LSTM
from keras.layers import Input
from keras.models import Model

# representation of model layers
# from keras.utils.vis_utils import plot_model

# dataset doesn't have column names, so we have to provide it
col_names = ["duration","protocol_type","service","flag","src_bytes",
    "dst_bytes","land","wrong_fragment","urgent","hot","num_failed_logins",
    "logged_in","num_compromised","root_shell","su_attempted","num_root",
    "num_file_creations","num_shells","num_access_files","num_outbound_cmds",
    "is_host_login","is_guest_login","count","srv_count","serror_rate",
    "srv_serror_rate","rerror_rate","srv_rerror_rate","same_srv_rate",
    "diff_srv_rate","srv_diff_host_rate","dst_host_count","dst_host_srv_count",
    "dst_host_same_srv_rate","dst_host_diff_srv_rate","dst_host_same_src_port_rate",
    "dst_host_srv_diff_host_rate","dst_host_serror_rate","dst_host_srv_serror_rate",
    "dst_host_rerror_rate","dst_host_srv_rerror_rate","label","difficulty_level"]

drive_url = 'https://drive.google.com/file/d/1sZYIzHDQo91nEAPFOVvSRe8euDXtHc2m/view?usp=sharing'

file_id = drive_url.split('/d/')[1].split('/')[0]
download_url = f'https://drive.google.com/uc?id={file_id}'

# Download the file
gdown.download(download_url, 'IDS_Dataset.txt', quiet=False)

# importing dataset
data = pd.read_csv('IDS_Dataset.txt',header=None, names=col_names)

# print dataset
data

# remove attribute 'difficulty_level'
data.drop(['difficulty_level'],axis=1,inplace=True)
data.shape

# descriptive statistics of dataset
data.describe()

# number of attack labels
data['label'].value_counts()

# changing attack labels to their respective attack class
def change_label(df):
  df.label.replace(['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm'],'Dos',inplace=True)
  df.label.replace(['ftp_write','guess_passwd','httptunnel','imap','multihop','named','phf','sendmail',
       'snmpgetattack','snmpguess','spy','warezclient','warezmaster','xlock','xsnoop'],'R2L',inplace=True)
  df.label.replace(['ipsweep','mscan','nmap','portsweep','saint','satan'],'Probe',inplace=True)
  df.label.replace(['buffer_overflow','loadmodule','perl','ps','rootkit','sqlattack','xterm'],'U2R',inplace=True)

# calling change_label() function
change_label(data)

# distribution of attack classes
data.label.value_counts()

"""# **Data Normalization**"""

# selecting numeric attributes columns from data
numeric_col = data.select_dtypes(include='number').columns

# using standard scaler for normalizing
std_scaler = StandardScaler()
def normalization(df,col):
  df[col] = std_scaler.fit_transform(df[col])
  return df

# data before normalization
data.head()

# calling the normalization() function
data = normalization(data.copy(),numeric_col)

# data after normalization
data.head()

"""# **Encoding**"""

# selecting categorical data attributes
cat_col = ['protocol_type','service','flag']

# creating a dataframe with only categorical attributes
categorical = data[cat_col]
categorical.head()

# one-hot-encoding categorical attributes using pandas.get_dummies() function
categorical = pd.get_dummies(categorical,columns=cat_col)
categorical.head()

"""# **Binary Classification**"""

# changing attack labels into two categories 'normal' and 'abnormal'
bin_label = pd.DataFrame(data.label.map(lambda x:'normal' if x=='normal' else 'abnormal'))

# creating a dataframe with binary labels (normal,abnormal)
bin_data = data.copy()
bin_data['label'] = bin_label

# label encoding (0,1) binary labels (abnormal,normal)
le1 = preprocessing.LabelEncoder()
enc_label = bin_label.apply(le1.fit_transform)
bin_data['intrusion'] = enc_label

np.save("le1_classes.npy",le1.classes_,allow_pickle=True)

# dataset with binary labels and label encoded column
bin_data.head()

# one-hot-encoding attack label
bin_data = pd.get_dummies(bin_data,columns=['label'],prefix="",prefix_sep="")
bin_data['label'] = bin_label
bin_data

# pie chart distribution of normal and abnormal labels
plt.figure(figsize=(8,8))
plt.pie(bin_data.label.value_counts(),labels=bin_data.label.unique(),autopct='%0.2f%%')
plt.title("Pie chart distribution of normal and abnormal labels")
plt.legend()
# plt.savefig('Pie_chart_binary.png')
plt.show()

"""# **Multi-class Classification**"""

# creating a dataframe with multi-class labels (Dos,Probe,R2L,U2R,normal)
multi_data = data.copy()
multi_label = pd.DataFrame(multi_data.label)

# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)
le2 = preprocessing.LabelEncoder()
enc_label = multi_label.apply(le2.fit_transform)
multi_data['intrusion'] = enc_label

np.save("le2_classes.npy",le2.classes_,allow_pickle=True)

# one-hot-encoding attack label
multi_data = pd.get_dummies(multi_data,columns=['label'],prefix="",prefix_sep="")
multi_data['label'] = multi_label
multi_data

# pie chart distribution of multi-class labels
plt.figure(figsize=(8,8))
# get value counts and unique labels
label_counts = multi_data.label.value_counts()
labels = label_counts.index  # Use the index of value_counts for labels

# create the pie chart
plt.pie(label_counts, labels=labels, autopct='%0.2f%%')

plt.title('Pie chart distribution of multi-class labels')
plt.legend()
# plt.savefig('plots/Pie_chart_multi.png')
plt.show()

"""# **Feature Extraction using Pearson Correlation(Filter Method)** 97% Precise"""

# creating a dataframe with only numeric attributes of binary class dataset and encoded label attribute
numeric_bin = bin_data[numeric_col]
numeric_bin['intrusion'] = bin_data['intrusion']

# finding the attributes which have more than 0.5 correlation with encoded attack label attribute
corr= numeric_bin.corr()
corr_y = abs(corr['intrusion'])
highest_corr = corr_y[corr_y >0.5]
highest_corr.sort_values(ascending=True)

# selecting attributes found by using pearson correlation coefficient
numeric_bin = bin_data[['count','srv_serror_rate','serror_rate','dst_host_serror_rate','dst_host_srv_serror_rate',
                         'logged_in','dst_host_same_srv_rate','dst_host_srv_count','same_srv_rate']]

# joining the selected attribute with the one-hot-encoded categorical dataframe
numeric_bin = numeric_bin.join(categorical)
# then joining encoded, one-hot-encoded, and original attack label attribute
bin_data = numeric_bin.join(bin_data[['intrusion','abnormal','normal','label']])

# saving final dataset to disk
bin_data.to_csv("./bin_data.csv")
# final dataset for binary classification
bin_data

# creating a dataframe with only numeric attributes of multi-class dataset and encoded label attribute
numeric_multi = multi_data[numeric_col]
numeric_multi['intrusion'] = multi_data['intrusion']

# finding the attributes which have more than 0.5 correlation with encoded attack label attribute
corr = numeric_multi.corr()
corr_y = abs(corr['intrusion'])
highest_corr = corr_y[corr_y >0.5]
highest_corr.sort_values(ascending=True)

# selecting attributes found by using pearson correlation coefficient
numeric_multi = multi_data[['count','logged_in','srv_serror_rate','serror_rate','dst_host_serror_rate',
                        'dst_host_same_srv_rate','dst_host_srv_serror_rate','dst_host_srv_count','same_srv_rate']]

# joining the selected attribute with the one-hot-encoded categorical dataframe
numeric_multi = numeric_multi.join(categorical)
# then joining encoded, one-hot-encoded, and original attack label attribute
multi_data = numeric_multi.join(multi_data[['intrusion','Dos','Probe','R2L','U2R','normal','label']])

# saving final dataset to disk
multi_data.to_csv('./multi_data.csv')

# final dataset for multi-class classification
multi_data

X = bin_data.iloc[:,0:93] # dataset excluding target attribute (encoded, one-hot-encoded,original)
Y = bin_data[['intrusion']] # target attribute

"""# **Feature Extraction using Particle Swarm Optimization** 47% Precise

"""

pip install pyswarms

print(encoded_data.dtypes)

from sklearn.preprocessing import LabelEncoder

# Make a clean copy
encoded_data = bin_data.copy()

# Drop columns that are not features
encoded_data = encoded_data.drop(['abnormal', 'normal', 'label'], axis=1)

# Encode categorical columns
categorical_cols = ['protocol_type', 'service', 'flag']
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    encoded_data[col] = le.fit_transform(encoded_data[col])
    label_encoders[col] = le

# Final check: ensure only numeric columns remain
assert all(encoded_data.dtypes != 'object'), "There are still non-numeric columns in the data!"

# Split into features and target
X_full = encoded_data.drop('intrusion', axis=1).values
y_full = encoded_data['intrusion'].values

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pyswarms as ps

# Convert to NumPy arrays
X_full = encoded_data.iloc[:, 0:93].values
y_full = encoded_data['intrusion'].values

# Objective function: return (1 - accuracy) for minimization
def fitness_function(particles):
    n_particles = particles.shape[0]
    scores = []
    for i in range(n_particles):
        mask = particles[i] > 0.5  # binary mask
        if np.sum(mask) == 0:
            scores.append(1)  # penalize empty feature subset
            continue
        X_subset = X_full[:, mask]
        X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_subset, y_full, test_size=0.25, random_state=42)
        clf = RandomForestClassifier()
        clf.fit(X_train_p, y_train_p)
        y_pred = clf.predict(X_test_p)
        acc = accuracy_score(y_test_p, y_pred)
        scores.append(1 - acc)  # because PSO minimizes
    return np.array(scores)

# Dimensions = number of features
dimensions = X_full.shape[1]
options = {
    'c1': 0.5,  # cognitive parameter
    'c2': 0.3,  # social parameter
    'w': 0.9,   # inertia
    'k': 3,     # number of neighbors (can be 1â€“5)
    'p': 2      # use Euclidean distance (p=2)
}

optimizer = ps.discrete.BinaryPSO(n_particles=20, dimensions=dimensions, options=options)

# Run optimization
best_cost, best_pos = optimizer.optimize(fitness_function, iters=20)

# Get the selected features
selected_features = np.where(best_pos == 1)[0]
print("Selected feature indices:", selected_features)

# Filter only the selected features
X = encoded_data.iloc[:, selected_features]
Y = encoded_data[['intrusion']]

"""# **Long Short-Term Memory Classifier (Binary Classification)**"""

# splitting the dataset 75% for training and 25% testing
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)
X_train = X_train.values
y_train = np.array(y_train)
x_train = np.reshape(X_train, (X_train.shape[0],1,X_train.shape[1]))
x_train.shape

lst = Sequential()
# lst.add(LSTM(50, input_shape=(1, X_train.shape[1]))) # Uncomment if PSO used
lst.add(LSTM(50, input_shape=(1, 93))) # Assuming timesteps=1 Uncomment if Pearson is used

lst.add(Dense(1, activation='sigmoid'))
lst.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# summary of model layers
lst.summary()

# Convert X_train and y_train to numeric types
x_train = x_train.astype(np.float64)  # Convert to float64
y_train = y_train.astype(np.float64)  # Convert to float64 or np.int64 if appropriate

# training the model on training dataset
history = lst.fit(x_train, y_train, epochs=50, batch_size=5000,validation_split=0.2)

import os  # Import os module for file path handling
from tensorflow.keras.models import model_from_json

# Define file paths
filepath = './lst_binary.json'
weightspath = './lst_binary.weights.h5'

# Check if the model file exists
if not os.path.isfile(filepath):
    # Serialize model to JSON
    lst_json = lst.to_json()
    with open(filepath, "w") as json_file:
        json_file.write(lst_json)
    print(f"Saved model JSON to {filepath}")

    try:
        # Serialize weights to HDF5
        lst.save_weights(weightspath)
        print(f"Saved weights to {weightspath}")
    except Exception as e:
        print(f"Error saving weights: {e}")
else:
    print(f"Model JSON already exists at {filepath}")

# Check if weights file exists before loading
if os.path.isfile(weightspath):
    print(f"Loading weights from {weightspath}")
    # Load json and create model
    json_file = open(filepath, 'r')
    loaded_model_json = json_file.read()
    json_file.close()

    lst = model_from_json(loaded_model_json)
    lst.load_weights(weightspath)
    print("Loaded model and weights from disk")
else:
    print(f"Error: Weights file not found at {weightspath}. Ensure the weights are saved correctly.")

# load json and create model
json_file = open(filepath, 'r')
loaded_model_json = json_file.read()
json_file.close()
lst = model_from_json(loaded_model_json)

# load weights into new model
lst.load_weights(weightspath)
print("Loaded model from disk")

# defining loss function, optimizer, metrics and then compiling model
lst.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

# predicting target attribute on testing dataset
X_test = X_test.astype(np.float64)  # Convert to float64
x_test = np.reshape(X_test, (X_test.shape[0],1,X_test.shape[1]))
x_test = x_test.astype(np.float64)  # Convert to float64
test_results = lst.evaluate(x_test, y_test, verbose=1)
print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')

# Plot of accuracy vs epoch of train and test dataset
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title("Plot of accuracy vs epoch for train and test dataset")
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='best')
# plt.savefig('plots/lstm_binary_accuracy.png')
plt.show()

# Plot of loss vs epoch of train and test dataset
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title("Plot of loss vs epoch for train and test dataset")
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='best')
# plt.savefig('plots/lstm_binary_loss.png')
plt.show()

y_test.shape

print(x_test)

y_pred = lst.predict(x_test)
y_pred = np.nan_to_num(y_pred)  # Replace NaN with 0

print(y_pred)

y_pred = lst.predict(x_test)
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
from sklearn import metrics
auc = metrics.roc_auc_score(y_test, y_pred)

plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC curve')
plt.legend(loc='best')
# plt.savefig('plots/lstm_binary_roc.png')
plt.show()

y_classes = (lst.predict(x_test)>0.5).astype('int32')

print("Recall Score - ",recall_score(y_test,y_classes))
print("F1 Score - ",f1_score(y_test,y_classes))
print("Precision Score - ",precision_score(y_test,y_classes))
# Evaluate model
loss, accuracy = lst.evaluate(x_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Predict and show classification report
y_pred = (lst.predict(x_test) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))

"""# **CNN-LSTM Model**"""

from sklearn.preprocessing import MinMaxScaler
X = bin_data.iloc[:, 0:93].values
Y = bin_data[['intrusion']].values

# Scale features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Reshape for CNN
X_cnn = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_cnn, Y, test_size=0.2, random_state=42)

from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten

model_cnnlstm = Sequential()
model_cnnlstm.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X.shape[1], 1)))
model_cnnlstm.add(MaxPooling1D(pool_size=2))
model_cnnlstm.add(Flatten())
model_cnnlstm.add(Dense(64, activation='relu'))
model_cnnlstm.add(Dropout(0.3))
model_cnnlstm.add(Dense(1, activation='sigmoid'))
model_cnnlstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train CNN-LSTM
# Use X_train and y_train which are now consistent in size
model_cnnlstm.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1)

"""# **Ensemble Learning**"""

pred_lstm = lst.predict(X_test)
pred_cnnlstm = model_cnnlstm.predict(X_test_cnn)
ensemble_pred = (pred_lstm + pred_cnnlstm) / 2
ensemble_labels = (ensemble_pred > 0.5).astype(int)

print("âœ… Ensemble Accuracy:", accuracy_score(y_test, ensemble_labels))
print("\nðŸ“Š Classification Report:\n", classification_report(y_test, ensemble_labels))

cm = confusion_matrix(y_test, ensemble_labels)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Ensemble Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()