# -*- coding: utf-8 -*-
"""Copy of FYP_IDS_NSL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fUW3ORecEwHDeG7TDzY1LMbOP6gqU6VW
"""

import sys
import keras
import sklearn
import itertools
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn import metrics
import sklearn.preprocessing
from scipy.stats import zscore
import matplotlib.pyplot as plt
from keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import get_file, plot_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
from keras.layers import Dense, Dropout, Activation, Embedding
from keras.layers import LSTM, SimpleRNN, GRU, Bidirectional, BatchNormalization,Convolution1D,MaxPooling1D, Reshape, GlobalAveragePooling1D
from sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score, confusion_matrix
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import Perceptron
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import AdaBoostClassifier
import sklearn.metrics as metrics
from sklearn import svm, datasets
from sklearn.utils.multiclass import unique_labels
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
# from scipy import interp
from itertools import cycle
from sklearn.datasets import make_classification
from sklearn.neighbors import KNeighborsClassifier
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('KDDTrain+.txt', header=None)
df.head()

qp = pd.read_csv('KDDTest+.txt', header=None)
qp.head()

df.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',
'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',
'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',
'su_attempted', 'num_root', 'num_file_creations', 'num_shells',
'num_access_files', 'num_outbound_cmds', 'is_host_login',
'is_guest_login', 'count', 'srv_count', 'serror_rate',
'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',
'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',
'dst_host_srv_count', 'dst_host_same_srv_rate','dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
'dst_host_srv_serror_rate', 'dst_host_rerror_rate',
'dst_host_srv_rerror_rate', 'subclass', 'difficulty_level']
df.head()

qp.columns = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes',
'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',
'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',
'su_attempted', 'num_root', 'num_file_creations', 'num_shells',
'num_access_files', 'num_outbound_cmds', 'is_host_login',
'is_guest_login', 'count', 'srv_count', 'serror_rate',
'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',
'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',
'dst_host_srv_count', 'dst_host_same_srv_rate','dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
'dst_host_srv_serror_rate', 'dst_host_rerror_rate',
'dst_host_srv_rerror_rate', 'subclass', 'difficulty_level']
qp.head()

lst_names = df.columns # column names
lst_names

testlst_names = qp.columns
testlst_names

df = df.drop('difficulty_level', axis=1)
df.shape

qp = qp.drop('difficulty_level', axis=1)
qp.shape

df.isnull().values.any()

qp.isnull().values.any()

# Distribution graphs (histogram/bar graph) of column data
def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):
    nunique = df.nunique()
    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]]
    nRow, nCol = df.shape
    columnNames = list(df)
    # Ensure nGraphRow is an integer using floor division //
    nGraphRow = (nCol + nGraphPerRow - 1) // nGraphPerRow
    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')
    for i in range(min(nCol, nGraphShown)):
        plt.subplot(nGraphRow, nGraphPerRow, i + 1)
        columnDf = df.iloc[:, i]
        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):
            valueCounts = columnDf.value_counts()
            valueCounts.plot.bar()
        else:
            columnDf.hist()
        plt.ylabel('counts')
        plt.xticks(rotation = 90)
        plt.title(f'{columnNames[i]} (column {i})')
    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)
    plt.savefig('1')
    plt.show()

plotPerColumnDistribution(df, 25, 5)

# Correlation matrix
def plotCorrelationMatrix(df, graphWidth):

    # df = df.dropna('columns') #Incorrect usage - should be a keyword argument
    #df = df.dropna(axis='columns') # Correct Usage: Specify axis as a keyword argument

    # Select only numeric features for correlation calculation
    numeric_df = df.select_dtypes(include=np.number)

    # Drop columns with NaN values before calculating correlation
    #numeric_df = numeric_df.dropna(axis='columns') # Correct Usage: Specify axis as a keyword argument
    numeric_df = numeric_df.dropna(axis=1)

    df = df[[col for col in df if df[col].nunique() > 1]]
    if numeric_df.shape[1] < 2:
        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')
        return
    corr = numeric_df.corr() # Calculate correlation for numeric columns only

    # Check if corr is empty
    if corr.empty:
        print("Correlation matrix is empty. Cannot plot.")
        return

    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')
    corrMat = plt.matshow(corr, fignum = 1)
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.columns)), corr.columns)
    plt.gca().xaxis.tick_bottom()
    plt.colorbar(corrMat)
    plt.title(f'Correlation Matrix for NSL-KDD Train', fontsize=15)
    plt.savefig('2')
    plt.show()

label_counts = df['subclass'].value_counts()
plt.figure(figsize=(18,6));
sns.barplot(y=label_counts.index, x=label_counts.values, color='Yellow');
plt.title('values per class');
plt.savefig('3')
# display(label_counts)

#binary traffic proportions
binary_class = []
for label in df['subclass']:
    if label !='normal':
        binary_class.append('malicious')
    else:
        binary_class.append('normal')
binary_class = pd.Series(binary_class)
plt.figure()
binary_class.value_counts().plot(kind='pie', label='traffic proportions', autopct='%.2f%%' )
plt.savefig('4')
plt.show()

attack_vs_protocol = pd.crosstab(df.subclass, df.protocol_type)
attack_vs_protocol

# Multiple Charts - Helper Function
def pie_charts(data_list,labels,name):
    list_length = len(data_list)

    color_list = sns.color_palette()
    color_cycle = itertools.cycle(color_list)
    cdict = {}

    fig, axs = plt.subplots(1, list_length,figsize=(18,10), tight_layout=False)
    plt.subplots_adjust(wspace=1/list_length)

    for count, data_set in enumerate(data_list):

        for num, value in enumerate(np.unique(data_set.index)):
            if value not in cdict:
                cdict[value] = next(color_cycle)

        wedges,texts = axs[count].pie(data_set,
                           colors=[cdict[v] for v in data_set.index])

        axs[count].legend(wedges, data_set.index,
                           title="Flags",
                           loc="center left",
                           bbox_to_anchor=(1, 0, 0.5, 1))

        axs[count].set_title(labels[count])
        plt.savefig(name)

    return axs

# series for each protocol
icmp_attacks = attack_vs_protocol.icmp
tcp_attacks = attack_vs_protocol.tcp
udp_attacks = attack_vs_protocol.udp

pie_charts([icmp_attacks, tcp_attacks, udp_attacks],['icmp','tcp','udp'], '5')
plt.show()

# Normal - 0, Attacks - 1
is_attack = df.subclass.map(lambda a: 0 if a == 'normal' else 1)
df['attack_flag'] = is_attack

# count of each flag for attack and normal traffic
normal_flags = df.loc[df.attack_flag == 0].flag.value_counts()
attack_flags = df.loc[df.attack_flag == 1].flag.value_counts()

flag_axs = pie_charts([normal_flags, attack_flags], ['normal','attack'], '6')
plt.show()

# count of each service for attack and normal traffic
normal_services = df.loc[df.attack_flag == 0].service.value_counts()
attack_services = df.loc[df.attack_flag == 1].service.value_counts()

service_axs = pie_charts([normal_services, attack_services], ['normal','attack'], '7')
plt.show()

df = df.drop('attack_flag', axis = 1)
df.columns

cols = ['protocol_type','service','flag']
cols

def one_hot(df, cols):

    for each in cols:
        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)
        df = pd.concat([df, dummies], axis=1)
        # Use 'columns' with a list of column labels to drop
        df = df.drop(columns=[each]) # Or df = df.drop(each, axis=1)
    return df

combined_data = pd.concat([df,qp])

combined_data = one_hot(combined_data,cols)

def normalize(df, cols):
    result = df.copy()
    for feature_name in cols:
        # Convert boolean columns to integers before normalization
        if result[feature_name].dtype == bool:
            result[feature_name] = result[feature_name].astype(int)

        # Convert the feature column to numeric if it is of type 'object'
        if result[feature_name].dtype == 'object':
            result[feature_name] = pd.to_numeric(result[feature_name], errors='coerce')

        # Now that everything is numeric or NaN, do the normalization
        if pd.api.types.is_numeric_dtype(result[feature_name]):
            max_value = result[feature_name].max()
            min_value = result[feature_name].min()

            # Check if the column has more than one unique value to avoid division by zero
            if max_value > min_value:
                result[feature_name] = (result[feature_name] - min_value) / (max_value - min_value)
            else:
                print(f"Skipping normalization for column with constant values: {feature_name}")
        else:
            print(f"Skipping normalization for non-numeric column: {feature_name}")

    return result

tmp = combined_data.pop('subclass')

new_train_df = normalize(combined_data,combined_data.columns)
new_train_df

classlist = []
check1 = ("apache2","back","land","neptune","mailbomb","pod","processtable","smurf","teardrop","udpstorm","worm")
check2 = ("ipsweep","mscan","nmap","portsweep","saint","satan")
check3 = ("buffer_overflow","loadmodule","perl","ps","rootkit","sqlattack","xterm")
check4 = ("ftp_write","guess_passwd","httptunnel","imap","multihop","named","phf","sendmail","Snmpgetattack","spy","snmpguess","warezclient","warezmaster","xlock","xsnoop")

DoSCount=0
ProbeCount=0
U2RCount=0
R2LCount=0
NormalCount=0

for item in tmp:
    if item in check1:
        classlist.append("DoS")
        DoSCount=DoSCount+1
    elif item in check2:
        classlist.append("Probe")
        ProbeCount=ProbeCount+1
    elif item in check3:
        classlist.append("U2R")
        U2RCount=U2RCount+1
    elif item in check4:
        classlist.append("R2L")
        R2LCount=R2LCount+1
    else:
        classlist.append("Normal")
        NormalCount=NormalCount+1

new_train_df["Class"] = classlist
new_train_df

new_train_df["Class"].value_counts()

new_train_df.isnull().values.any()

y_train=new_train_df["Class"]
y_train

y_train.isnull().values.any()

combined_data_X = new_train_df.drop('Class', axis= 1)
combined_data_X

batch_size = 32
model = Sequential()
# Change 'border_mode' to 'padding' and use 'same' as the value
model.add(Convolution1D(64, kernel_size=122, padding="same",activation="relu",input_shape=(122, 1)))
# Use 'pool_size' instead of 'pool_length'
model.add(MaxPooling1D(pool_size=5))
model.add(BatchNormalization())
model.add(Bidirectional(LSTM(64, return_sequences=False)))
model.add(Reshape((128, 1), input_shape = (128, )))

# Use 'pool_size' instead of 'pool_length'
model.add(MaxPooling1D(pool_size=5))
model.add(BatchNormalization())
model.add(Bidirectional(LSTM(128, return_sequences=False)))

model.add(Dropout(0.5))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

for layer in model.layers:
    print(layer.output.shape) # Use layer.output.shape to access the output shape

model.summary()

oos_pred = []
k_dict = {'accuracy' : [], 'detectionRate' : [], 'falsepositiverate' : [], 'confusionmatrix' : []}

def build_model(input_shape, num_classes):
    model = Sequential()
    model.add(Convolution1D(64, kernel_size=122, padding="same", activation="relu", input_shape=input_shape))
    model.add(MaxPooling1D(pool_size=5))
    model.add(BatchNormalization())
    model.add(Bidirectional(LSTM(64, return_sequences=False)))
    model.add(Reshape((128, 1)))
    model.add(MaxPooling1D(pool_size=5))
    model.add(BatchNormalization())
    model.add(Bidirectional(LSTM(128, return_sequences=False)))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Split data once
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

train_X, test_X, train_y, test_y = train_test_split(combined_data_X, y_train, test_size=0.2, random_state=42, stratify=y_train)

# Encode labels
le = LabelEncoder()
y_train_encoded = to_categorical(le.fit_transform(train_y))
y_test_encoded = to_categorical(le.transform(test_y))

# Reshape inputs
x_train_array = train_X.values.reshape(train_X.shape[0], train_X.shape[1], 1)
x_test_array = test_X.values.reshape(test_X.shape[0], test_X.shape[1], 1)

# Build model
model = build_model((x_train_array.shape[1], 1), y_train_encoded.shape[1])

# Train
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
model.fit(x_train_array, y_train_encoded, validation_data=(x_test_array, y_test_encoded), epochs=10, batch_size=64, callbacks=[early_stop])

# Save
model.save("ids_lstm_model_final.h5")

# K-Fold training and model saving
for k in range(2,11,2):
    k_dict['accuracy'].append([])
    k_dict['confusionmatrix'].append([])
    kfold = StratifiedKFold(n_splits=k,shuffle=True,random_state=42)
    kfold.get_n_splits(combined_data_X,y_train)
    fold_idx = 0  # Track fold index for saving
    for train_index, test_index in kfold.split(combined_data_X,y_train):
        train_X, test_X = combined_data_X.iloc[train_index], combined_data_X.iloc[test_index]
        train_y, test_y = y_train.iloc[train_index], y_train.iloc[test_index]

        x_columns_train = new_train_df.columns.drop('Class')
        x_train_array = train_X[x_columns_train].values
        x_train_1=np.reshape(x_train_array, (x_train_array.shape[0], x_train_array.shape[1], 1))

        dummies = pd.get_dummies(train_y)
        outcomes = dummies.columns
        num_classes = len(outcomes)
        y_train_1 = dummies.values

        x_columns_test = new_train_df.columns.drop('Class')
        x_test_array = test_X[x_columns_test].values
        x_test_2=np.reshape(x_test_array, (x_test_array.shape[0], x_test_array.shape[1], 1))

        dummies_test = pd.get_dummies(test_y)
        outcomes_test = dummies_test.columns
        num_classes = len(outcomes_test)
        y_test_2 = dummies_test.values
        early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
        model.fit(x_train_1, y_train_1, validation_data=(x_test_2, y_test_2), epochs=5, callbacks=[early_stop])


        # Save the model for this fold
        model.save(f"ids_lstm_model_k{k}_fold{fold_idx}.h5")
        fold_idx += 1

        pred = model.predict(x_test_2)
        pred = np.argmax(pred,axis=1)
        y_eval = np.argmax(y_test_2,axis=1)
        score = metrics.accuracy_score(y_eval, pred)
        k_dict['accuracy'][-1].append(score)
        cm = confusion_matrix(y_eval, pred, labels=[0,1,2,3,4])
        k_dict['confusionmatrix'][-1].append(cm)
        print("Validation score: {}".format(score))

accuracy = []
for k_acc in k_dict['accuracy']:
    accuracy.append(max(k_acc))

f1_scores = []
for k_mat in k_dict['confusionmatrix']:
    f1_scores.append([])
    for i in range(len(k_mat)):
        f1 = []
        for j in range(5):
            pr = (k_mat[i][j][j])/sum(k_mat[i][j])
            s = 0
            for m in range(5):
                s+=k_mat[i][m][j]
            re = (k_mat[i][j][j])/s
            score = (2 * pr * re)/ (pr + re)
            f1.append(score)
        f1_scores[-1].extend(f1)


class_f1_scores = []
k = 2
for i in range(5):
    class_f1_scores.append([])
    for j in range(5):
        s = sum(f1_scores[i][j::5])
        class_f1_scores[-1].append(s/k)
    k+=2

false_positive_rate = []
detection_rate = []
for k_mat in k_dict['confusionmatrix']:
    false_positive_rate.append([])
    detection_rate.append([])
    for i in range(len(k_mat)):
        FP = k_mat[i].sum(axis=0) - np.diag(k_mat[i])
        FN = k_mat[i].sum(axis=1) - np.diag(k_mat[i])
        TP = np.diag(k_mat[i])
        TN = k_mat[i].sum() - (FP + FN + TP)

        FPR = sum(FP)/sum(FP+TN)
        DR = sum(TP)/sum(TP + FP)
        false_positive_rate[-1].append(FPR)
        detection_rate[-1].append(DR)

    false_positive_rate[-1] = np.mean(false_positive_rate[-1])*100
    detection_rate[-1] = np.mean(detection_rate[-1])

# Detection Rate Plot
k_vals = [2,4,6,8,10]
plt.figure()
plt.plot(k_vals, detection_rate, marker='o', label = 'Detection Rate')
plt.title('Detection Rate Plot')
plt.xlabel('K-value')
plt.ylabel('Detection Rate')
plt.legend()
plt.savefig('Detection Rate Plot')
plt.show()

# False Positive Rate Plot
plt.figure()
plt.plot(k_vals, false_positive_rate, marker='o', label = 'False Positive Rate')
plt.title('False Positive Rate Plot')
plt.xlabel('K-value')
plt.ylabel('False Positive Rate')
plt.legend()
plt.savefig('False Positive Rate Plot')
plt.show()

# F1 Score plot of all classes
DoS_F1, Normal_F1, Probe_F1, R2L_F1, U2R_F1 = [], [], [], [], []
for i in range(5):
    DoS_F1.append(class_f1_scores[i][0])
    Normal_F1.append(class_f1_scores[i][1])
    Probe_F1.append(class_f1_scores[i][2])
    R2L_F1.append(class_f1_scores[i][3])
    U2R_F1.append(class_f1_scores[i][4])

plt.figure()
plt.plot(k_vals, DoS_F1, marker='o', label = 'DoS')
plt.plot(k_vals, Normal_F1, marker='o', label = 'Normal')
plt.plot(k_vals, Probe_F1, marker='o', label = 'Probe')
plt.plot(k_vals, R2L_F1, marker='o', label = 'R2L')
plt.plot(k_vals, U2R_F1, marker='o', label = 'U2R')
plt.title('F1-score plot of all classes')
plt.xlabel('k-Value')
plt.ylabel('F1-score')
plt.legend()
plt.savefig('F1-score Plot')
plt.show()

# Confusion Matrix Plot
class_names = ["DoS","Normal","Probe","R2L","U2R"]
cmap=plt.cm.Blues
cm = k_dict['confusionmatrix'][4][7]
fig, ax = plt.subplots()
im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
ax.figure.colorbar(im, ax=ax)
ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),
           xticklabels=class_names, yticklabels=class_names,
           title='Confusion Matrix',
           ylabel='True label',
           xlabel='Predicted label')

plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")

thresh = cm.max() / 2.
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(j, i, format(cm[i, j]), ha="center", va="center",
                color="white" if cm[i, j] > thresh else "black")
fig.tight_layout()
plt.savefig('Confusion Matrix')
plt.show()

np.set_printoptions(precision=2)

# Table of Multiclass Classification Results
d = [['2', accuracy[0], detection_rate[0], false_positive_rate[0]], ['4', accuracy[1], detection_rate[1], false_positive_rate[1]], ['6', accuracy[2], detection_rate[2], false_positive_rate[2]], ['8', accuracy[3], detection_rate[3], false_positive_rate[3]], ['10', accuracy[4], detection_rate[4], false_positive_rate[4]], ['Average', np.mean(accuracy), np.mean(detection_rate), np.mean(false_positive_rate)]]
tab = pd.DataFrame(d,columns = ['K-value','Accuracy%','Detection Rate%','False Positive Rate%'])
tab

"""end of testing"""

trainingdata = new_train_df.values
X_train = trainingdata[:125973,:-1]
Y_train = trainingdata[:125973,-1]

X_test = trainingdata[125973:,:-1]
Y_test = trainingdata[125973:,-1]

#PLA
clf = Perceptron (tol=1e-3)
clf.fit(X_train, Y_train)
train_acc = clf.score(X_train, Y_train)
test_acc = clf.score(X_test, Y_test)
y_pred = clf.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

PLA_f1score = f1_score(Y_test, y_pred, average="macro")
PLA_precision = precision_score(Y_test, y_pred, average="macro")
PLA_recall = recall_score(Y_test, y_pred, average="macro")
PLA_accuracy = accuracy_score(Y_test, y_pred)

#Logistic Regression
clf = LogisticRegression (solver='liblinear', multi_class='auto')
clf.fit(X_train, Y_train)
train_acc = clf.score(X_train, Y_train)
test_acc = clf.score(X_test, Y_test)
y_pred = clf.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

Logistic_f1 = f1_score(Y_test, y_pred, average="macro")
Logistic_precision = precision_score(Y_test, y_pred, average="macro")
Logistic_recall = recall_score(Y_test, y_pred, average="macro")
Logistic_accuracy = accuracy_score(Y_test, y_pred)

#Neural Network
clf = MLPClassifier (hidden_layer_sizes =(4,6,8))
clf = MLPClassifier (hidden_layer_sizes =(200,))
clf.fit(X_train, Y_train)
train_acc = clf.score(X_train, Y_train)
test_acc = clf.score(X_test, Y_test)
y_pred = clf.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

Neural_f1 = f1_score(Y_test, y_pred, average="macro")
Neural_precision = precision_score(Y_test, y_pred, average="macro")
Neural_recall = recall_score(Y_test, y_pred, average="macro")
Neural_accuracy = accuracy_score(Y_test, y_pred)

#Decision Tree
clf = DecisionTreeClassifier()
clf = clf.fit(X_train, Y_train)
train_acc = clf.score(X_train, Y_train)
test_acc = clf.score(X_test, Y_test)
y_pred = clf.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

Tree_f1 = f1_score(Y_test, y_pred, average="macro")
Tree_precision = precision_score(Y_test, y_pred, average="macro")
Tree_recall = recall_score(Y_test, y_pred, average="macro")
Tree_accuracy = accuracy_score(Y_test, y_pred)

#Uniform Aggregation for PLA, logistic regression, NN and decision trees
clf1 = Perceptron (tol=1e-3)
clf2 = LogisticRegression (solver='liblinear', multi_class='auto')
clf3 = MLPClassifier (hidden_layer_sizes =(4,6,8))
clf4 = DecisionTreeClassifier()

eclf1 = VotingClassifier(estimators=[('PLA', clf1), ('LR', clf2), ('NN', clf3),('Tree',clf4)], voting='hard')
eclf1 = eclf1.fit(X_train,Y_train)
train_acc = eclf1.score(X_train, Y_train)
test_acc = eclf1.score(X_test, Y_test)
y_pred = eclf1.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

import joblib

# Save all classical models
joblib.dump(clf1, 'perceptron_model.pkl')
joblib.dump(clf2, 'logistic_model.pkl')
joblib.dump(clf3, 'mlp_model.pkl')
joblib.dump(clf4, 'decision_tree_model.pkl')
joblib.dump(eclf1, 'voting_classifier.pkl')

# Re-train Bagging and AdaBoost (or reuse if already trained)
joblib.dump(BaggingClassifier(estimator=clf1).fit(X_train, Y_train), 'bagging_model.pkl')
joblib.dump(AdaBoostClassifier(clf4).fit(X_train, Y_train), 'adaboost_model.pkl')

# Random Forest (or use existing clf if already fitted)
rf_clf = RandomForestClassifier().fit(X_train, Y_train)
joblib.dump(rf_clf, 'random_forest_model.pkl')

Uniform_f1 = f1_score(Y_test, y_pred, average="macro")
Uniform_precision = precision_score(Y_test, y_pred, average="macro")
Uniform_recall = recall_score(Y_test, y_pred, average="macro")
Uniform_accuracy = accuracy_score(Y_test, y_pred)

#Bagging with PLA
PLA_clf = Perceptron (tol=1e-3)

# Use 'estimator' instead of 'base_estimator'
clf = BaggingClassifier(estimator = PLA_clf)
clf = clf.fit(X_train,Y_train)
train_acc = clf.score(X_train, Y_train)
test_acc = clf.score(X_test, Y_test)
y_pred = clf.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

Bagging_f1 = f1_score(Y_test, y_pred, average="macro")
Bagging_precision = precision_score(Y_test, y_pred, average="macro")
Bagging_recall = recall_score(Y_test, y_pred, average="macro")
Bagging_accuracy = accuracy_score(Y_test, y_pred)

#AdaBoost with decision tree
decision_clf = DecisionTreeClassifier()
clf = AdaBoostClassifier(decision_clf)
clf.fit(X_train,Y_train)
train_acc = clf.score(X_train, Y_train)
test_acc = clf.score(X_test, Y_test)
y_pred = clf.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

Ada_f1 = f1_score(Y_test, y_pred, average="macro")
Ada_precision = precision_score(Y_test, y_pred, average="macro")
Ada_recall = recall_score(Y_test, y_pred, average="macro")
Ada_accuracy = accuracy_score(Y_test, y_pred)

#Random Forest
clf = RandomForestClassifier()
clf.fit(X_train,Y_train)
train_acc = clf.score(X_train, Y_train)
test_acc = clf.score(X_test, Y_test)
y_pred = clf.predict(X_test)
print("Training accuracy is:", train_acc )
print("Testing accuracy is:", test_acc)

RandomForest_f1 = f1_score(Y_test, y_pred, average="macro")
RandomForest_precision = precision_score(Y_test, y_pred, average="macro")
RandomForest_recall = recall_score(Y_test, y_pred, average="macro")
RandomForest_accuracy = accuracy_score(Y_test, y_pred)

data = [["PLA",PLA_accuracy,PLA_precision,PLA_recall,PLA_f1score],["Logistic Regression",Logistic_accuracy,Logistic_precision,Logistic_recall,Logistic_f1],["NN",Neural_accuracy,Neural_precision,Neural_recall,Neural_f1],["DTree",Tree_accuracy,Tree_precision,Tree_recall,Tree_f1],["Voting",Uniform_accuracy,Uniform_precision,Uniform_recall,Uniform_f1],["Bagging of PLA",Bagging_accuracy,Bagging_precision,Bagging_recall,Bagging_f1],["AdaBoost",Ada_accuracy,Ada_precision,Ada_recall,Ada_f1],["Random Forest",RandomForest_accuracy,RandomForest_precision,RandomForest_recall,RandomForest_f1]]
rt = pd.DataFrame(data,columns = ['','Mean Acc','Mean Precision','Mean Recall','Mean F1'])
rt

class_names = ["DoS","Normal","Probe","R2L","U2R"]

classifier = Perceptron (tol=1e-3)
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None,cmap=plt.cm.Blues):

    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'


    cm = confusion_matrix(y_true, y_pred)


    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax


np.set_printoptions(precision=2)

#PLA confusion matrix
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='PLA matrix, without normalization')
plt.show()

#Logistic Regression Matrix
classifier = LogisticRegression (solver='liblinear', multi_class='auto')
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='Logistic Regression matrix, without normalization')
plt.show()

#NN confusion matrix
classifier = MLPClassifier (hidden_layer_sizes =(4,6,8))
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='Neural Network matrix, without normalization')
plt.show()

#Confusion Matrix for decision tree
classifier = DecisionTreeClassifier()
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='Decision Tree, without normalization')
plt.show()

classifier = VotingClassifier(estimators=[('PLA', clf1), ('LR', clf2), ('NN', clf3),('Tree',clf4)], voting='hard')
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='Uniform Aggregation, without normalization')
plt.show()

#Bagging with PLA confusion matrix
# Use 'estimator' instead of 'base_estimator'
classifier = BaggingClassifier(estimator = PLA_clf)
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='Bagging with PLA, without normalization')
plt.show()

#Adaboost Confusion matrix

classifier = AdaBoostClassifier(decision_clf)
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='Adaboost , without normalization')
plt.show()

#Random Forest Confusion matrix

classifier = RandomForestClassifier()
y_pred = classifier.fit(X_train, Y_train).predict(X_test)
plot_confusion_matrix(Y_test, y_pred, classes=class_names,title='Random Forest, without normalization')
plt.savefig('RFCM')
plt.show()

# --- Save both models at the end ---

# Save CNN-LSTM model
model.save("cnn_lstm_model.h5")
print("✅ CNN-LSTM model saved as cnn_lstm_model.h5")

# Save LSTM model
model_rnn.save("lstm_model.h5")
print("✅ LSTM model saved as lstm_model.h5")